{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "The approximate depth of a DT trained on a training set of a million instances is\n",
    "log2(1000000) ~ 20\n",
    "\n",
    "### Exercise 2\n",
    "A node's Gini impurity is generally lower than it's parent. For example, a Node with a set {A, B, A, A, A} has G of 1 - (1/ 25 + 16/25) = 0.32. If the Node is split between {A, B} and {A, A, A}, we would have Gs of 0.5 and 0 respectively. \n",
    "\n",
    "### Exercise 3\n",
    "If a DT if overfitting a training set, it is a good idea to reduce the max_depth. Or to increase the min_samples_leaf.\n",
    "\n",
    "### Exercise 4\n",
    "If a DT is underfitting a training set, do not try to scale the input_features. It will not do anything. Try increasing the max_depth, or decreasing the min_samples_leaf.\n",
    "\n",
    "### Exercise 5\n",
    "If we have a million training instances that takes an hour to train, a set with 10 million instances would take \n",
    "K = (n*10m*log(10m))/(n*m*log(m)) \n",
    "  = 10*log(10m)/log(m)\n",
    "  = 10 * log(10 * 1000000) / log(1000000)\n",
    "  ~ 11.67 hours\n",
    "  \n",
    "### Exercise 6\n",
    "Presorting a training set with 100,000 instances would make training slower because the fastest sorting algorithm would be O(m*log(m)). So presorting is only better when m*log(m) < n. Great for datasets with picture as training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
      "            max_features=None, max_leaf_nodes=24,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=42,\n",
      "            splitter='best')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86280000000000001"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 7\n",
    "## Train and fine tune a DT for the moons dataset.\n",
    "## 1) Generate a moons dataset using make_moons(n_samples=10000, \n",
    "##    noise = 0.4)\n",
    "## 2) Split in into a training and test set using train test split.\n",
    "## 3) use grid search with cross-validation (with help of the \n",
    "##    GridSearchCV class) to find good hyper values for a DTC.\n",
    "## 4) Train it on the full training set using these hyper parameters\n",
    "##    and measure your model's performance on the test set. ~86%\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.4, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "parameters_test = {\n",
    "    'max_leaf_nodes': [10, 50, 90, 100],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_leaf': range(1, 10),\n",
    "    'min_samples_split': range(2, 10),\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "}\n",
    "\n",
    "parameters_good = {\n",
    "    'max_leaf_nodes': [23, 24, 25, 26, 27],\n",
    "    'max_depth': [5],\n",
    "    'min_samples_leaf': [1],\n",
    "    'min_samples_split': [2],\n",
    "    'criterion': ['gini'],\n",
    "}\n",
    "dt_clf = DecisionTreeClassifier(random_state=42)\n",
    "clf = GridSearchCV(dt_clf, parameters_good)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print(clf.best_estimator_)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500, 1)\n",
      "(2500,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86280000000000001"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exercise 7\n",
    "## Grow a fucking forest!\n",
    "## 1) Generate 1000 subsets of 100 of the training set randomly \n",
    "##    (ShuffleSplit)\n",
    "## 2) Train 1 DT on each subset. Evaluate the DTs on the test set.\n",
    "##    They will perform worse than the normal one.\n",
    "## 3) Pull predictions on the 1000 models and use the mode to make\n",
    "##    a predictions.\n",
    "## 4) Evaluate the predictions!\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from scipy.stats import mode\n",
    "\n",
    "rs = ShuffleSplit(n_splits=1000, test_size=0)\n",
    "\n",
    "trees = []\n",
    "for train_index, _ in rs.split(X_train):\n",
    "    trees.append(DecisionTreeClassifier(class_weight=None, \n",
    "            criterion='gini', max_depth=5,\n",
    "            max_features=None, max_leaf_nodes=24,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, presort=False, splitter='best'))\n",
    "    trees[len(trees)-1].fit(X_train[train_index], y_train[train_index])\n",
    "\n",
    "preds = []\n",
    "for tree in trees:\n",
    "    preds.append(tree.predict(X_test))\n",
    "\n",
    "mode_pred, _ = mode(preds)\n",
    "\n",
    "print(mode_pred.T.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "accuracy_score(y_test, mode_pred.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86519999999999997"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=1000, class_weight=None, \n",
    "            criterion='gini', max_depth=5,\n",
    "            max_features=None, max_leaf_nodes=24,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "y_preds = rf_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
