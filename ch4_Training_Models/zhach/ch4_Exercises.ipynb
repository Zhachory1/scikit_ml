{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What Linear Regression training algorithm can you use if you have a training set with millions of features? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic or Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose the features in your training set have very different scales. What algorithms might suffer from this and how? What can you do about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Gradient Descent algorithms would take a long time to converge. If the features are at different scales, the derivative (or slope) at a given point would be very different when compared to features with similar scales. Because of this, it would take a long time for the algorithm to converge.  \n",
    "\n",
    "You can solve this by Scaling all the feature down to either the [0, 1] or [-1, 1] scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can Gradient Descent get stuck in a local minimum when training a Logistic Regression Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it cannot. The cost function for Logistic Regression is convex (bowl like) so it's local minimum is it's global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all Gradient Descent algorithms lead to the same model provided you let them run long enough?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoritically, yes, as long the learning rate is not too high. If the learning rate is set correctly, Batch will always converge onto it. However Stochastic and Mini-batch has a touch of randomness set upon it. It's really hard for it to converge onto the optimum model. However, if the time constraints were infinite, then yes, there is a chance they would converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you use Batch Gradient Descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Either the model is overfitting the training data or your learning rate is too high. To see what the problem is, you must plot the training error as well. If they are both going up, lower the learning rate. If training is going down and validation is going up, the model is overfitting. You can fix this by tweaking hyperparameters, adding more training data, and/or add regularization to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a good idea to stop Mini-batch Gradient Descent immediately when the validation error goes up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No it is not. Just because the error rose doesnt necessarily mean that it reached it's global minimum. Especially with the random nature of Mini-batch. It's a better idea to stop whenever the validation error goes above a certain margin respective to the previously found minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which Gradient Descent algorithm will reach the vicinity of the optimal solution the fastest? Will it actually converge? How can you make the others converge as well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic will most likely get you near the optimal solution the fastest, but it will most definitely not converge. However if you add a good learning schedule on them, it might have an easier time converging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are using Polynomial Regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model overfit. Don't dumb. Less poly. More regular. Much data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are using Ridge Regression and you notice that the training error and the validation error are almost equal and very high. Would you say the model suffers from high bias or high variance? Should you increase alpha or reduce it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It most likely suffers from bias. It's underfitting the data. So you should reduce regularization/alpha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why would you want to use:\n",
    "* Ridge instead of plain Linear?\n",
    "* Lasso over Ridge?\n",
    "* Elastic over Lasso?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I would use ridge prefereably over linear because it's always good to have a little bit of regularization.\n",
    "* I would use lasso if I knew that some of the feature could be taken out.\n",
    "* I would use elastic over lasso because lasso can be quite erratic when # of features outweigh # of samples or when several features are strongly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you want to classify pictures outdoor/indoor and daytime/nighttime. Should you implement 2 Logistic Regression Classifiers or one Softmax Regression Classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would use 2 Logistic Regression Classifiers because the are not exclusive classes. Meaning it can be daytime AND you can be outside. However, you can combine the classes to have 4 classes like \"day-out\", \"day-in\", \"night-out\", and \"night-in.\" At that point you can use Softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement Batch Gradient Descent with early stopping for Softmax Regression (without Scikit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta:  [[ 0.6403056   0.88418567  0.73619156]\n",
      " [ 0.13851707  0.01628565  0.75837502]\n",
      " [ 0.18859991  0.13391533  0.98557961]\n",
      " [ 0.46350179  0.11164916  0.88768417]\n",
      " [ 0.3172996   0.89023537  0.01653488]]\n",
      "0.379776740026 0.392655030539\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCncRkQakcBcRaUAKdxGRBqRwFxFpQP8PXzZL+5FHT94AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff164b55e90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "data = iris[\"data\"]\n",
    "target = iris[\"target\"]\n",
    "X_b = np.c_[data, np.ones((data.shape[0], 1))]\n",
    "\n",
    "# ONLY USING THIS FOR LABEL MANIP! NOT ACTUAL MACHINE LEARNING!\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(target)\n",
    "\n",
    "rands = np.random.permutation(X_b.shape[0])\n",
    "test_size = int(len(data) * 0.1)\n",
    "test_ixs = rands[:test_size]\n",
    "train_ixs = rands[test_size:]\n",
    "X_train, X_test = X_b[train_ixs], X_b[test_ixs]\n",
    "y_train, y_test = y[train_ixs], y[test_ixs]\n",
    "\n",
    "# Thetas are in columns\n",
    "theta = np.random.random((X_train.shape[1], y_train.shape[1]))\n",
    "print \"Theta: \", theta\n",
    "\n",
    "def sm_pred(X):\n",
    "    proba = sm_proba(X)\n",
    "    return np.argmax(proba, axis=1)\n",
    "\n",
    "def sm_proba(X):\n",
    "    score = theta.T.dot(X.T)\n",
    "    total_scores = np.sum(score, axis=0).reshape(-1, 1)\n",
    "    return (score.T / total_scores)\n",
    "\n",
    "def sm_cost(X, y):\n",
    "    # Basically log-loss\n",
    "    proba = sm_proba(X)\n",
    "    m = X.shape[0]\n",
    "    if (proba < 0.0).any():\n",
    "        negative = (proba < 0.0)\n",
    "        proba[negative] = 1e-15\n",
    "    y_times_log_p = y * np.log(proba)\n",
    "    return (y_times_log_p.sum() / m)\n",
    "    \n",
    "def sm_grad_descent(X, y):\n",
    "    proba = sm_proba(X)\n",
    "    m = X.shape[0]\n",
    "    pre_grads =(proba - y).T.dot(X)\n",
    "    return pre_grads.T / m\n",
    "    \n",
    "eta = 0.5\n",
    "previous_validation_score = 100000\n",
    "validation_score = 99999\n",
    "\n",
    "train_err = []\n",
    "val_err = []\n",
    "\n",
    "# Start BGD\n",
    "while True:\n",
    "    gradients = sm_grad_descent(X_train, y_train)\n",
    "    theta = theta - eta * gradients\n",
    "    train_err.append(-sm_cost(X_train, y_train))\n",
    "    previous_validation_score = validation_score\n",
    "    validation_score = -sm_cost(X_test, y_test)\n",
    "    val_err.append(validation_score)\n",
    "    if abs(validation_score - previous_validation_score) < 1e-8:\n",
    "        break\n",
    "print train_err[-1], val_err[-1]\n",
    "plt.plot(np.arange(0, len(train_err)), train_err, 'r', np.arange(0, len(train_err)), val_err, 'b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 2 2 1 1 2 0 2 1] [0 0 2 2 1 2 2 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "print sm_pred(X_test[:10]), np.argmax(y_test[:10], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
