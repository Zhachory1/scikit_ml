{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Tensorflow:', '1.1.0')\n",
      "(u'Python Version:', '2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 17:05:42) \\n[GCC 7.2.0]')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from datetime import datetime\n",
    "from functools import partial\n",
    "from itertools import compress\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "import sys\n",
    "print(\"Python Version:\", sys.version)\n",
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "def new_logdir():\n",
    "    now = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "    root_logdir = \"tf_logs\"\n",
    "    return \"{}/run-{}/\".format(root_logdir, now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Is it okay to initialize all the weights to the same value as long as that value is selected randomly using He Initialization?\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 2\n",
    "Is it okay to initialize the bias terms to 0?\n",
    "\n",
    "\n",
    "\n",
    "# Exercise 3\n",
    "Name 3 advantages of the ELU activation function over the ReLU?\n",
    "1. It has a constant derivative over the entire X range\n",
    "2. Bloop\n",
    "3. Bloop\n",
    "\n",
    "# Exercise 4\n",
    "In which cases would you want to use each of the following activation functions?\n",
    "1. ELU:\n",
    "2. Leaky ReLU (and varients):\n",
    "3. ReLU: \n",
    "4. Tanh:\n",
    "5. Logistic:\n",
    "6. Softmax: to use on the output layer to output probabilities.\n",
    "\n",
    "# Exercise 5\n",
    "What may happen is you set the momentum hyperparameter too close to 1 when using the `MomentOptimizer`?\n",
    "\n",
    "It won't actually take the gradients into account. Similar to using a very high learning rate, it might over jump to optimum minimum and take a long time to converge.\n",
    "\n",
    "# Exercise 6\n",
    "Name three ways you can produce a sparse model?\n",
    "1. You can trim weights to 0 when it's below a certain threshold.\n",
    "2. You can use a strong L1 regularizer (allows 0 weights).\n",
    "3. You can use FTLR, which is a crazy algorithm that comes up with the weights on the fly based on other parameters.\n",
    "\n",
    "# Exercise 7 \n",
    "Does dropout slow down training? Does it slow down inference? \n",
    "\n",
    "It does slow down training because it will take a while for it to converge, but it shouldn't slow down inference because we are not performing dropout at that stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 1 1 0]\n",
      "[7 6 8 9 8]\n"
     ]
    }
   ],
   "source": [
    "X_train = mnist.train.images\n",
    "X_test = mnist.test.images\n",
    "y_train = mnist.train.labels.astype(\"int\")\n",
    "y_test = mnist.test.labels.astype(\"int\")\n",
    "\n",
    "keep_l4_train = np.array([x <= 4 for x in y_train])\n",
    "keep_l4_test = np.array([x <= 4 for x in y_test])\n",
    "\n",
    "X_train_l4 = X_train[keep_l4_train]\n",
    "X_test_l4 = X_test[keep_l4_test]\n",
    "y_train_l4 = y_train[keep_l4_train]\n",
    "y_test_l4 = y_test[keep_l4_test]\n",
    "\n",
    "X_train_g5 = X_train[np.invert(keep_l4_train)]\n",
    "X_test_g5 = X_test[np.invert(keep_l4_test)]\n",
    "y_train_g5 = y_train[np.invert(keep_l4_train)]\n",
    "y_test_g5 = y_test[np.invert(keep_l4_test)]\n",
    "\n",
    "print(y_train_l4[:5])\n",
    "print(y_train_g5[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "# 1) DNN with 5L[100]N, He, and ELU\n",
    "# 2) Adam, and Early stopping. \n",
    "# 3) Save model and checkpoints.\n",
    "# 4) Train on 0-4 MNIST\n",
    "# 5) Tune with cross-validation. Precision?\n",
    "# 6) Batch-Normalization. Compare?\n",
    "# 7) Overfitting? Use Drop. How about now?\n",
    "\n",
    "n_inputs = 28*28\n",
    "layer_sizes = [100, 100, 100, 100, 100]\n",
    "n_outputs = 5\n",
    "\n",
    "lr = 0.01\n",
    "\n",
    "he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "elu = tf.nn.elu\n",
    "my_dense_layer = partial(tf.layers.dense, activation=elu, kernel_initializer=he_init)\n",
    "\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "    \n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden = []\n",
    "    last_hidden = X\n",
    "    for i in xrange(len(layer_sizes)):\n",
    "        hidden.append(my_dense_layer(last_hidden, layer_sizes[i], name=\"hidden\"+str(i)))\n",
    "        last_hidden = hidden[-1]\n",
    "    logits = tf.layers.dense(last_hidden, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "    \n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    acc_sum = tf.summary.scalar(\"Accuracy\", accuracy)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "print(\"Ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(X, y, batch_size):\n",
    "    idxs = np.random.randint(0, len(X), batch_size)\n",
    "    return X[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train accuracy: 0.694 Test accuracy: 0.668418\n",
      "10 Train accuracy: 0.922 Test accuracy: 0.929169\n",
      "20 Train accuracy: 0.95 Test accuracy: 0.958552\n",
      "30 Train accuracy: 0.956 Test accuracy: 0.962833\n",
      "40 Train accuracy: 0.974 Test accuracy: 0.972368\n",
      "50 Train accuracy: 0.968 Test accuracy: 0.974509\n",
      "60 Train accuracy: 0.978 Test accuracy: 0.978984\n",
      "70 Train accuracy: 0.97 Test accuracy: 0.976844\n",
      "80 Train accuracy: 0.974 Test accuracy: 0.98093\n",
      "90 Train accuracy: 0.978 Test accuracy: 0.981125\n",
      "100 Train accuracy: 0.972 Test accuracy: 0.982876\n",
      "110 Train accuracy: 0.988 Test accuracy: 0.983265\n",
      "120 Train accuracy: 0.988 Test accuracy: 0.982681\n",
      "130 Train accuracy: 0.986 Test accuracy: 0.981125\n",
      "140 Train accuracy: 0.984 Test accuracy: 0.980541\n",
      "150 Train accuracy: 0.982 Test accuracy: 0.972174\n",
      "160 Train accuracy: 0.988 Test accuracy: 0.981319\n",
      "170 Train accuracy: 0.982 Test accuracy: 0.981903\n",
      "180 Train accuracy: 0.986 Test accuracy: 0.984627\n",
      "190 Train accuracy: 0.984 Test accuracy: 0.985406\n",
      "200 Train accuracy: 0.984 Test accuracy: 0.983849\n",
      "210 Train accuracy: 0.99 Test accuracy: 0.987157\n",
      "220 Train accuracy: 0.976 Test accuracy: 0.986768\n",
      "230 Train accuracy: 0.982 Test accuracy: 0.988714\n",
      "INFO:tensorflow:Restoring parameters from ./ckpts/epoch_230.ckpt\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "best_threshold = 0.001\n",
    "\n",
    "log_dir = new_logdir()\n",
    "train_writer = tf.summary.FileWriter(log_dir + '/train', tf.get_default_graph())\n",
    "test_writer = tf.summary.FileWriter(log_dir + '/test', , tf.get_default_graph())\n",
    "\n",
    "acc_train = 0\n",
    "acc_test = 0\n",
    "acc_test_best = 0\n",
    "steps_since_best = 0\n",
    "best_ckpt = \"\"\n",
    "epoch = 0\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    while steps_since_best < 100:\n",
    "        X_batch, y_batch = make_batch(X_train_l4, y_train_l4, batch_size)\n",
    "        sess.run(train, feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_train, train_sum = sess.run(\n",
    "            [accuracy, acc_sum],\n",
    "            feed_dict={X:X_batch, y:y_batch})\n",
    "        acc_test, test_sum = sess.run(\n",
    "            [accuracy, acc_sum],\n",
    "            feed_dict={X:X_test_l4, y:y_test_l4})\n",
    "        if epoch % 10 == 0:\n",
    "            train_writer.add_summary(train_sum, epoch)\n",
    "            test_writer.add_summary(test_sum, epoch)\n",
    "            print(epoch, \"Train accuracy:\", acc_train, \"Test accuracy:\", acc_test)\n",
    "        steps_since_best += 1\n",
    "        epoch += 1\n",
    "        if acc_test_best < acc_test:\n",
    "            if acc_test_best + best_threshold < acc_test:\n",
    "                steps_since_best = 0\n",
    "            acc_test_best = acc_test\n",
    "            best_ckpt = saver.save(sess, \"./ckpts/epoch_\" + str(epoch) + \".ckpt\")\n",
    "    saver.restore(sess, best_ckpt)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
