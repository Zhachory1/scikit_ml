{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'Tensorflow:', '1.1.0')\n",
      "(u'Python Version:', '2.7.14 |Anaconda, Inc.| (default, Dec  7 2017, 17:05:42) \\n[GCC 7.2.0]')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime\n",
    "from sklearn.datasets import load_iris,fetch_mldata\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score, log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "print(\"Tensorflow:\", tf.__version__)\n",
    "\n",
    "import sys\n",
    "print(\"Python Version:\", sys.version)\n",
    "\n",
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = b\"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "What are the advantages of a CNN over a fully connected DNN for image classification?\n",
    "\n",
    "1. CNN weights are only partially connected. Less weights to train so its faster to train, less trianing data, and reduce the risk of overfitting. \n",
    "2. CNNs have an idea of nearby pixels. DNNs don't. \n",
    "3. Once a CNN learns feature in one part of an image, it can find that same feature on other parts of an image.\n",
    "\n",
    "# Exercise 2\n",
    "Consider a CNN composed of 3 convolutional layers, each with 3x3 kernels, a stride of 2, and SAME padding. The lowest layer outputs 100 feature maps, the middle one outputs 200, and the top one outputs 400. The input images are RGB images of 200x300 pixels. What is the total number of parameters in the CNN? If we are using 32-bit floats, at least how much RAM will this network require when making a prediction for a single instance? What about training on a mini-batch of 50 images?\n",
    "\n",
    "Welp...first. The first layer has 3x3 kernels with 3 maps, plus a bias. That all together is 28 parameters per kernel. Times the 100 feature maps is 2,800 parameters for the first layer. For the second layer, we have the same kinda 3x3 with now 100 maps from the last layer, plus a bias. That together is 901 per kernel. Times the 200 feature maps is 180,200 for the second layer. For the third layer, we have 3x3 kernels for 200 maps from the last layer, plus a bias. Together, that is 1,801. Times the 400 feature maps is 720,400 parameters. Add all those together, we have 2,800 + 180,200 + 720,400 = 903,400.\n",
    "\n",
    "For RAM space for predicting 1 image, it's gonna be a bit more complicated. Since we are using a stride of 2, we are shrinking the image by 2 each time. So a 200x300 turns into a 100x150 after the first layer, a 50x75 after the second layer, and 25x38 after the third. A float is a 32-bit, or 4 byte, value. Times the feature maps of the first layer, we get 4x100x150x100 = 6,000,000 bytes. The second layer is 4x50x75x200 = 3,000,000 bytes. The last, following the pattern here, is near 1,500,000 bytes. But TF freees memory when it needs to, so we just have to compute the maximum of two adjacent layers which is 9,000,000 million bytes. Plus the memory it takes to store  the 903,400 parameters (3,613,300 bytes), is 12,613,300 bytes or around 12 MB.\n",
    "\n",
    "Training on a minibatch is more intense. TF stores values of each instance for back propagation. So we multipy 50 x (6,000,000 + 3,000,000 + 1,500,000), which is 525,000,000 bytes. We add that to the size of all 50 images, 50 x (3x200x300) x 4 = 36,000,000. We add all of that with the size of the parameters, which is 3,613,300 bytes. Plus gradient variables, but they get initialize and destroyed fairly quickly, so we can assume the maximum would be the size of the max layer, 720,400x4 or 2,881,600 bytes. Add all of those together and we get 567,494,900 bytes or about 541 MB. \n",
    "\n",
    "# Exercise 3\n",
    "If your GPU runs out of memory while training a CNN, what are 5 things you could try to solve the problem.\n",
    "\n",
    "1. Fucking lower that batch size, ya twat.\n",
    "2. Lower the number of layers. \n",
    "3. Larger strides to make the image smaller per layer\n",
    "4. Use 16-bit floats\n",
    "5. Distribute CNN\n",
    "\n",
    "# Exercise 4\n",
    "Why would you want to add a max pooling layer rather than a convolutional layer with the same stride?\n",
    "\n",
    "To subsample (or shrink) the input image to reduce computational load, memory usage, and the number of parameters.\n",
    "\n",
    "# Exercise 5\n",
    "When would you want to add a *local response normalization* layer?\n",
    "\n",
    "When you don't want feature maps to learn similar features, most likely implemented in the lower layers of a CNN to generalize better. It encourages different maps to specialize and push them apart and force them to explore a wider range of features.\n",
    "\n",
    "# Exercise 6\n",
    "Innovations of AlexNet, compared to LeNet-5? What about GoogLeNet and ResNet.\n",
    "\n",
    "1. AlexNet stacked Conv layers right on top of each other and used LRNs to normalize feature maps.\n",
    "2. GoogLeNet created subnetworks calls inception modules wthat distributed the input across independent layers to be joined later by a concat operator.\n",
    "3. ResNet first introduced the idea of skip connections, bringing the output of a layer up 2 to 4 layers to the input of another to speed up training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build your own CNN and try to achieve the highest possible \n",
    "## accuracy on MNIST.\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.001\n",
    "\n",
    "reset_graph()\n",
    "# Inputs\n",
    "with tf.name_scope(\"inputs\"):\n",
    "    X = tf.placeholder(tf.float32, shape=(None, 784), name=\"X\")\n",
    "    X_reshape = tf.reshape(X, [-1, 28, 28, 1])\n",
    "    y = tf.placeholder(tf.int64, shape=(None), name=\"y\")\n",
    "    training = tf.Variable(True)\n",
    "\n",
    "# Actual model\n",
    "with tf.name_scope(\"model\"):\n",
    "    with tf.name_scope(\"cnn\"):\n",
    "        conv1 = tf.layers.conv2d(inputs=X_reshape, \n",
    "                                 filters=32, \n",
    "                                 kernel_size=(5,5),\n",
    "                                 padding=\"same\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(conv1, \n",
    "                                        pool_size=(2,2), \n",
    "                                        strides=2)\n",
    "        batch1 = tf.layers.batch_normalization(pool1, \n",
    "                                               training=training, \n",
    "                                               axis=1)\n",
    "        conv2 = tf.layers.conv2d(inputs=batch1, \n",
    "                                 filters=64, \n",
    "                                 kernel_size=(5,5),\n",
    "                                 padding=\"same\",\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2, \n",
    "                                        pool_size=(2,2), \n",
    "                                        strides=2)\n",
    "        batch2 = tf.layers.batch_normalization(pool2, \n",
    "                                               training=training, \n",
    "                                               axis=1)\n",
    "    \n",
    "    with tf.name_scope(\"fully_connected\"):\n",
    "        flat = tf.reshape(batch2, [-1, 7*7*64])\n",
    "        dense1 = tf.layers.dense(inputs=flat, \n",
    "                                 units=1024,\n",
    "                                 activation=tf.nn.relu)\n",
    "        drop = tf.layers.dropout(inputs=dense1, training=training)\n",
    "        logits = tf.layers.dense(inputs=drop, units=10)\n",
    "        predictions = {\n",
    "            \"classes\": tf.argmax(logits, axis=1),\n",
    "            \"probabilities\": tf.nn.softmax(logits, name=\"softmax\")\n",
    "        }\n",
    "\n",
    "# Loss functions \n",
    "with tf.name_scope(\"eval\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "# Holds training ops\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.RMSPropOptimizer(learning_rate=lr)\n",
    "    train_op = optimizer.minimize(loss=loss)\n",
    "\n",
    "# Eval Metrics\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    pre_precision = tf.metrics.precision(y, predictions['classes'])\n",
    "    acc = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    pre = tf.reduce_mean(tf.cast(pre_precision, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST-data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST-data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST-data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.contrib.learn.datasets.load_dataset(\"mnist\")\n",
    "train_data = mnist.train.images\n",
    "train_labels = mnist.train.labels\n",
    "test_data = mnist.test.images\n",
    "test_labels = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_batch(X, y, size):\n",
    "    idxs = np.random.randint(0, len(X), batch_size)\n",
    "    return X[idxs], y[idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: (0.090000004, 0.1118) Precision: (0.46470588, 0.96264905)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-165965202621>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mbest_ckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./ckpts/epoch_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mepoch\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0msteps_since_best\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/zhach/miniconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 10\n",
    "grab_metrics = 50\n",
    "best_threshold = 0.001\n",
    "num_of_mets = n_epochs // grab_metrics\n",
    "\n",
    "train_pre = []\n",
    "train_acc = []\n",
    "test_pre = []\n",
    "test_acc = []\n",
    "\n",
    "init_g = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "acc_test_best = 0\n",
    "steps_since_best = 0\n",
    "best_ckpt = \"\"\n",
    "epoch = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_g)\n",
    "    sess.run(init_l)\n",
    "    while epoch < n_epochs and steps_since_best < 200:\n",
    "        X_tr, y_tr = make_batch(train_data, \n",
    "                                train_labels, \n",
    "                                batch_size)\n",
    "        if epoch % grab_metrics == 0:\n",
    "            tr_p, tr_a, _ = sess.run([pre, acc, train_op], \n",
    "                                     feed_dict={X: X_tr, y: y_tr})\n",
    "            te_p, te_a = sess.run([pre, acc], \n",
    "                                  feed_dict={X:test_data,\n",
    "                                             y:test_labels,\n",
    "                                             training:False})\n",
    "            print(\"Accuracy:\", (tr_a, te_a),\n",
    "                  \"Precision:\", (tr_p, te_p))\n",
    "            train_pre.append(tr_p)\n",
    "            train_acc.append(tr_a)\n",
    "            test_pre.append(te_p)\n",
    "            test_acc.append(te_a)\n",
    "            if acc_test_best < test_acc[-1]:\n",
    "                if acc_test_best + best_threshold < test_acc[-1]:\n",
    "                    steps_since_best = 0\n",
    "                acc_test_best = test_acc[-1]\n",
    "                best_ckpt = saver.save(sess, \"./ckpts/epoch_\" + str(epoch) + \".ckpt\")\n",
    "        else:\n",
    "            sess.run(train_op, feed_dict={X: X_tr, y: y_tr})\n",
    "        epoch += 1\n",
    "        steps_since_best += 1\n",
    "    saver.restore(sess, best_ckpt)\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_acc, color=\"blue\")\n",
    "plt.plot(test_acc, color=\"red\")\n",
    "plt.show()\n",
    "plt.plot(train_pre, color=\"blue\")\n",
    "plt.plot(test_pre, color=\"red\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 8"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
