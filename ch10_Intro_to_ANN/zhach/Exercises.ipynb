{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    "Draw an ANN using the original artificial neurons (like the ones in Fig10-3) that computes `A XOR B`. Hint  `A XOR B = (A AND NOT B) OR (NOT A AND B)`.\n",
    "\n",
    "![alt](https://i.stack.imgur.com/nRZ6z.png)\n",
    "\n",
    "\n",
    "# Exercise 2\n",
    "Why is it generally preferable to use a Logistic Regression rather than a classical Perceptron? How can you tweak the Perceptron to make it equivalent to a Logistic Regression Classifier?\n",
    "\n",
    "Perceptrons do not output a class probability, they just make predictions based on a hard threshold. Logistic Regressors do, which makes them more flexible and preferable.\n",
    "\n",
    "\n",
    "# Exercise 3\n",
    "Why was the logistic activation function a key ingredient in trian the first MLPs?\n",
    "\n",
    "With backpropagation gaining speed, researchers needed a better activation function than the step function. Step function didnt have any gradients to implement a gradient descent algorithm. The logistic function has a well-defined nonzero derivative everywhere, allowing GD to blossom into a beautiful little butterfly.\n",
    "\n",
    "\n",
    "# Exercise 4\n",
    "Name three popular activation function. Can you draw them?\n",
    "\n",
    "Tanh ![alt](https://upload.wikimedia.org/wikipedia/commons/thumb/c/cb/Activation_tanh.svg/120px-Activation_tanh.svg.png)\n",
    "Sigmoid (Logistic) ![alt](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5b/Activation_logistic.svg/120px-Activation_logistic.svg.png)\n",
    "ReLU ![alt](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Activation_rectified_linear.svg/120px-Activation_rectified_linear.svg.png)\n",
    "\n",
    "\n",
    "# Exercise 5\n",
    "Suppose you have an MLP composed of one input layer with 10 passthrough neurons, followed by one hidden layer with 50 artifical neurons, and finally one output layer with 3 neurons. All neurons use the ReLU activation functions.\n",
    "1. What is the shape of the input matrix X? (?, 10)\n",
    "2. What is the shape of the hidden layer's weight vector Wh, and the shape of it's bias vector, Bh? W.shape = (10, 50), B.shape = (1, 50)\n",
    "3. What is the shape of the output layers W and B? W.shape = (50, 3), B.shape = (1, 3)\n",
    "4. What is the shape of the network's output matrix Y? (?, 3)\n",
    "5. Write the equation that computs the networks output matrix Y as a function of X, Wh, Bh, Wo, and Bo. `Y = (X.Wh + Bh).Wo + Bo`\n",
    "\n",
    "\n",
    "# Exercise 6\n",
    "How many neurons do you need in the output layer if you want to classify email into Spam or Ham. What activation function should you use in the output layer? If instead you want to tackle MNIST, how many neurons do you need in the output layer, using what activations function. Answer the same questions for getting your network to predict housing prices as in Chapter 2.\n",
    "\n",
    "1. To classify between Spam and Ham, you would only need 1 output neuron (to say if it is spam or not) with a normal sigmoid.\n",
    "2. To classify the MNIST, you would need 10 neurons with a softmax.\n",
    "3. To predict housing prices, you would only need one neuron with a ReLU. A trick you can do is train the model to predict the log of the house prices, then exp() the result to infer prices. This will help the network not have crazy weights and stuff.\n",
    "\n",
    "\n",
    "# Exercise 7\n",
    "What is backpropagation and how does it work? What is the difference between backpropagation and reverse-mode autodiff?\n",
    "\n",
    "Backprogation is the process in which a neural network is train. The partial derivates of the cost function is calculated and propagtion backwords through the network to change the weights towards a global minima. Reverse-mode autodiff is just a process of how to get partial derivates from a computation graph given input values. It first pushes the value through the graph, bottom up, then computes the gradients going down the graph. BP uses RMAD.\n",
    "\n",
    "\n",
    "# Exercise 8\n",
    "Can you list all the hyperparameters you can tweak in an MLP? If the MLP over-fits the training data, how could you tweak these hyperparameters to try to solve the problem?\n",
    "\n",
    "You can change the learning rate, the number of layers, the number of neurons on each individual layer, the activation functions. If your model is overfitting the data, you can try to lower the learning rate or simlpifying the model by lowering the number of layers, lowering the number of neurons per layer, or change sqitch up the activation functions to see which would be better suited for your application. \n",
    "\n",
    "\n",
    "# Exercise 9\n",
    "See end of chapter 10 walkthrough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
